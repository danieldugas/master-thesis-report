\chapter{SegMatchAE}
\label{chap:segmatchAE}

This Chapter details the work following the implementation of SegMatch. In this part of the project, the goal was to design and implement a learning descriptor, that would improve the quality of descriptions and matches.\\

\section{Extending Segment Matching}
\label{sec:ae-intro}
%TODO

Recent work is showing neural networks as a versatile and promising machine learning technique. Despite initial promise in the years following the invention of the Perceptron \cite{perceptron} in 1958, neural networks did not show useful results, or superiority to other machine learning algorithms on many problems for some time. In the recent years (2006-) however, breakthrough results in the fields of audio (see WaveNet \cite{wavenet}), visual (see ? \cite{inception?}), natural language processing (see ? \cite{?}), game-agent (see AlphaGo \cite{alphago}), to cite a few, have led to increased research effort and adoption in consumer \cite{snapchat-face-recognition} and business \cite{google-cooling} technologies.\\

There is little doubt that neural networks can also be useful in the robotics field. With this in mind, and based on pre-existing research on classifying 3D segments using neural networks (see Subsection~\ref{subsec:voxnet}), it was decided that neural networks were an ideal test subject for improving SegMatch performance.\\

The description step was seen as showing the most potential for improvement. In addition, the description task shares similarities with 3D object classification, which is a current subject of machine learning research. Thus, the description step was selected as the section of the algorithm to be supplemented with a neural network model.\\

\section{Neural Networks for 3D Segment Recognition}
\label{sec:neural-nets}
%TODO

\subsection{Artificial Neural Networks - A Primer}
\label{subsec:primer}

\subsubsection{Simple Model}
\label{subsubsec:simple-model}

The simplest artificial neural network model is a series of layers, each containing a few artificial neurons. Inputs get propagated forwards through the network. Inspired by the way signals flow through a neuron's many synapses, each input to a neuron is multiplied by a particular weight. All of a neuron's weighted inputs are added together, a bias term is added, and the result is passed through the neuron's activation function $f(x)$, which can be for example the sigmoid function. The resulting value is the neuron's output.\\

All the neuron outputs of a layer become the inputs to all neurons in the next layer. This is called a \textit{fully-connected} layer architecture. The input values to the algorithm - are treated as though they were neuron outputs of an 'input' layer, and fed directly into the inputs of the first neuron layer.\\

The last neuron layer is called the 'output' layer, and its neurons activations form the output values of the algorithm. All layers between the 'input' and 'output' layer are referred to as 'hidden' layers.\\

In the simplest case, learning is achieved through \textit{backpropagation} of the output error. That is, the error between the model output and the target output (ideal output of the network, determined in advance for training purposes) is calculated. The gradient of this error w.r.t. the model parameters (weights and biases) is then computed, propagating backward from the output neurons all the way back to the first input layers. All parameters are then updated by a small amount according to the opposite direction of this gradient (this simple update rule is referred to as \textit{Gradient Descent}. When this process is repeated several times and for many examples, the output error is reduced.\\

\subsubsection{Deep Networks}

An arbitrary amount of hidden layers can theoretically be stacked behind each other. Networks consisting of many hidden layers are referred to as 'deep' neural networks, and are supposedly able to capture more complex relationships within their logical pathways than 'shallow' networks.\\

As is often repeated in the context of artificial neural networks, a shallow network consisting of a single neural network, if it has infinitely many neurons, can theoretically compute any function $f(inputs) = outputs$. In practice however, deeper networks are often used when attempting to comput complex functions.\\

However, deep neural networks used in such cases often have architectures and particularities which differ from the simple model presented in subsubsection \ref{subsubsec:simple-model}. This is due in part to instabilities which arise when backpropagating the error through many layers.\\ 

\subsubsection{Convolutional Neural Networks}

The intuition behind convolutional neural networks is to take advantage of repeating patterns in the model input. A neural network model which takes a whole image's pixel values as inputs, for example, is likely to be presented with underlying relationships in the input data which are repeated in several locations of the input.\\

Convolutional layers create small neural network layers, with a reduced amount of inputs $n\times n$ (for example, $n = 5$), commonly referred to as filters. A single filter is applied at each position in the full layer-input $N\times N$, taking the $n\times n$ surrounding values as filter inputs, and outputting a single value.\\

Filters can also be applied with strides, skipping over every k input positions, leading to a reduced size layer-output.\\

Thus convolutional layers allow useful pattern learning in the input, with few parameters. On the other hand, convolutions generally require heavier computation. They have shown good results in many tasks, and are quite widespread especially with tasks where repeated patterns are expected in the input (computer vision, audio).\\

\subsubsection{Other Architectures}

It should be mentioned that there are yet many other architectures of neural networks showing good results in current research, to name a few: Recurrent Neural Networks (RNN), Dynamic Neural Computers (DNC), Residual Networks (ResNets) and many more. These will not be discussed in this report.\\

\subsubsection{Optimizer}

Gradient Descent, mentioned in Subsubsection \ref{subsubsec:simple-model}, is a simple way, but not the most efficient, of finding local minima for the error function in the many-dimensional parameter space of the model.\\

Optimizers help navigate the many-dimensional parameter space more efficiently when performing the backpropagation step. They do so by implementing methods such as artificial momentum, looking ahead of the current position in this parameter space, and other techniques. This allows the backpropagation algorithm to reach local minima of the error function with fewer learning steps.\\

In this project, the neural networks were trained by using the ADAM optimizer. The Adaptive Moment Estimation (ADAM) optimizer computes learning rates for each parameter in the model, and adapts those rates based on previous fluctuations of gradients and previous parameter update values.\\

More information is available in the authors' paper \cite{adam}, where they also show empirical results of ADAM's learning speed-up. 

\subsection{Neural Networks for 3D Segment Classification}
\label{subsec:voxnet}
%TODO
VoxNet

\subsection{Unsupervised Learning with Neural Networks}
\label{subsec:autoencoder}
%TODO
Autoencoder

\subsection{Variational Bayes Autoencoder}
\label{subsec:variational-bayes}
%TODO

\section{Implementation}
\label{sec:ae-implementation}

%TODO
Performance
GPU Speedup

\section{Results}
\label{sec:ae-results}


